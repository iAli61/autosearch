{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM models:  ['gpt-4', 'gpt-4-32k', 'gpt-4-0613', 'gpt-35-turbo', 'gpt-35-turbo-16k']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, List, Optional, Union, Callable\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.formatting_utils import colored\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "\n",
    "from teachability import Teachability\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import arxiv\n",
    "\n",
    "project_name = \n",
    "\n",
    "db_dir = './teachability_db-4'\n",
    "# check if db_dir exists, delete it if it does\n",
    "import os\n",
    "import shutil\n",
    "if os.path.exists(db_dir): shutil.rmtree(db_dir)\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\", \"gpt-4\", \"gpt4\", \"gpt-35-turbo-16k\", \"gpt-4-0613\", \"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-35-turbo-0613\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"LLM models: \", [config_list[i][\"model\"] for i in range(len(config_list))])\n",
    "\n",
    "from typing import Annotated\n",
    "import concurrent.futures\n",
    "# Assuming other necessary imports like `autogen`, `Teachability`, etc., are done earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "import os\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teachable_groupchat(assitant_name, user_name, db_dir, config_list, verbosity=0):\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    assistant = autogen.ConversableAgent(\n",
    "        name=assitant_name,  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "    )\n",
    "\n",
    "    # Instantiate the Teachability capability. Its parameters are all optional.\n",
    "    teachability = Teachability(\n",
    "        verbosity=verbosity,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "        reset_db=False,  \n",
    "        path_to_db_dir=db_dir,\n",
    "        recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "    )\n",
    "\n",
    "    # Now add the Teachability capability to the agent.\n",
    "    teachability.add_to_agent(assistant)\n",
    "\n",
    "    user = autogen.UserProxyAgent(\n",
    "        name=user_name,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\"),\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "    )\n",
    "\n",
    "    return assistant, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Does any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "yes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Briefly copy any advice from the TEXT that may be useful for a similar but different task in the future. But if no advice is present, just respond with 'none'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Briefly copy just the task from the TEXT, then stop. Don't solve it, and don't include any advice.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Summarize very briefly, in general terms, the type of task described in the TEXT. Leave out details that might not appear in a similar problem.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "Evaluate the importance of content in a text and decide whether to remember it. If deemed valuable, take notes including the title and source link for reference.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[93m\n",
      "REMEMBER THIS TASK-ADVICE PAIR\u001b[0m\n",
      "\u001b[93m\n",
      "INPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n",
      "  ID\n",
      "    1\n",
      "  INPUT\n",
      "    Evaluate the importance of content in a text and decide whether to remember it. If deemed valuable, take notes including the title and source link for reference.\n",
      "  OUTPUT\n",
      "    Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\u001b[0m\n",
      "\u001b[92mLIST OF MEMOS\u001b[0m\n",
      "\u001b[92m  ID: 1\n",
      "    INPUT TEXT: Evaluate the importance of content in a text and decide whether to remember it. If deemed valuable, take notes including the title and source link for reference.\n",
      "    OUTPUT TEXT: Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\u001b[0m\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Does the TEXT contain information that could be committed to memory? Answer with just one word, yes or no.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "yes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Imagine that the user forgot this information in the TEXT. How would they ask you for this information? Include no other text in your response.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "What is the process I should follow for each memorization task, and how should I finalize my notes if I choose to memorize the passage?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \n",
      "\n",
      "' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33minstract_assistant\u001b[0m (to analyzer):\n",
      "\n",
      "Copy the information from the TEXT that should be committed to memory. Add no explanation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33manalyzer\u001b[0m (to instract_assistant):\n",
      "\n",
      "MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[93m\n",
      "REMEMBER THIS QUESTION-ANSWER PAIR\u001b[0m\n",
      "\u001b[93m\n",
      "INPUT-OUTPUT PAIR ADDED TO VECTOR DATABASE:\n",
      "  ID\n",
      "    2\n",
      "  INPUT\n",
      "    What is the process I should follow for each memorization task, and how should I finalize my notes if I choose to memorize the passage?\n",
      "  OUTPUT\n",
      "    MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
      "\u001b[0m\n",
      "\u001b[92mLIST OF MEMOS\u001b[0m\n",
      "\u001b[92m  ID: 1\n",
      "    INPUT TEXT: Evaluate the importance of content in a text and decide whether to remember it. If deemed valuable, take notes including the title and source link for reference.\n",
      "    OUTPUT TEXT: Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\u001b[0m\n",
      "\u001b[92m  ID: 2\n",
      "    INPUT TEXT: What is the process I should follow for each memorization task, and how should I finalize my notes if I choose to memorize the passage?\n",
      "    OUTPUT TEXT: MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': \"For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \\n\\n' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\", 'role': 'assistant'}, {'content': \"Understood. When you provide me with passages to memorize, I'll start my process as instructed. I'll evaluate the content, determine its value, and then memorize the key insights along with the article's title and URL as you've outlined. Please go ahead and provide the information you wish for me to memorize.\", 'role': 'user'}], summary=\"Understood. When you provide me with passages to memorize, I'll start my process as instructed. I'll evaluate the content, determine its value, and then memorize the key insights along with the article's title and URL as you've outlined. Please go ahead and provide the information you wish for me to memorize.\", cost=({'total_cost': 0.00867, 'gpt-4': {'cost': 0.00867, 'prompt_tokens': 157, 'completion_tokens': 66, 'total_tokens': 223}}, {'total_cost': 0.00867, 'gpt-4': {'cost': 0.00867, 'prompt_tokens': 157, 'completion_tokens': 66, 'total_tokens': 223}}), human_input=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \\n\\n' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\"\n",
    "\n",
    "instract_assistant, instract_user = create_teachable_groupchat(\"instract_assistant\", \"instract_user\", db_dir, config_list, verbosity=3)\n",
    "\n",
    "instract_user.initiate_chat(instract_assistant, silent=True, message=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_chat_with_paper_info(paper, teachable_agent, user, query_text):\n",
    "    user.initiate_chat(teachable_agent,\n",
    "                       silent=True,\n",
    "                       message=f\"The following article is one of the articles that I found for '{query_text}' topic: \\n\\n '{paper.title}' by {paper.authors} updated on {paper.updated}: {paper.pdf_url} \\nsummary: {paper.summary} \\n?\")\n",
    "\n",
    "def process_query(query_text, n_results, teachable_agent, user):\n",
    "    \"\"\"Function to process each query and initiate chats for each paper found.\"\"\"\n",
    "    sort_by = arxiv.SortCriterion.Relevance\n",
    "    papers = arxiv.Search(query=query_text, max_results=n_results, sort_by=sort_by)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_with_paper_info, paper, teachable_agent, user, query_text) for paper in arxiv.Client().results(papers)]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "def arxiv_retriever(queries: Annotated[List[str], \"The list of query texts to search for.\"], \n",
    "                    n_results: Annotated[int, \"The number of results to retrieve for each query.\"] = 10,\n",
    "                    ) -> str:\n",
    "    \n",
    "    # Create a TeachableAgent and UserProxyAgent to represent the researcher and the user, respectively.\n",
    "    arxiver, arxiver_user = create_teachable_groupchat(\"arxiver\", \"arxiver_user\", db_dir, config_list, verbosity=0)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_query, query_text, n_results, arxiver, arxiver_user) for query_text in queries]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    # Instantiate a UserProxyAgent to represent the user. But in this notebook, all user input will be simulated.\n",
    "    return f\"Dear Researcher, Database updated with on the following topics: {', '.join([query for query in queries])}. Please go ahead with your task. Contact me for updates if needed. Your research Coordinator\"\n",
    "\n",
    "\n",
    "message = [\"Overview of time series forecasting methods\", \"Deep learning for time series forecasting\"]\n",
    "# arxiv_retriever(message, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, save_path):\n",
    "    \"\"\"Download a PDF from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def initiate_chat_read_paper(text, teachable_agent, user, article):\n",
    "    user.initiate_chat(teachable_agent,\n",
    "                       silent=True,\n",
    "                       message=f\"MEMORIZE_ARTICLE: The following passage is extracted from an article titled '{article}': \\n\\n {text}.\"\n",
    "                    )\n",
    "\n",
    "\n",
    "def get_pdf(url: Annotated[str, \"The URL of the PDF to convert.\"]) -> str:\n",
    "\n",
    "    paper_reader, reader_user = create_teachable_groupchat(\"paper_reader\", \"reader_user\", db_dir, config_list, verbosity=0)\n",
    "    \n",
    "    output_dir = './pdf_output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    paper_id = url.split('/')[-1].replace('.pdf', '')\n",
    "    search_by_id = arxiv.Search(id_list=[paper_id])\n",
    "    paper = list(arxiv.Client().results(search_by_id))[0]\n",
    "    title = paper.title\n",
    "    \n",
    "    pdf_filename = url.split('/')[-1]\n",
    "    pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "    \n",
    "    download_pdf(url, pdf_path)\n",
    "    elements = partition_pdf(filename=pdf_path)\n",
    "    chunked_elements = chunk_by_title(elements)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_read_paper, chunk.text, paper_reader, reader_user, f\"{title} [{url}]\") for chunk in chunked_elements if len(chunk.text.split()) > 30]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "    \n",
    "    return f\"The article {title} [{url}] has been read and the content has been shared with you in your memory.\"\n",
    "\n",
    "# Example usage\n",
    "pdf_url = \"https://arxiv.org/ftp/arxiv/papers/2305/2305.17873.pdf\"\n",
    "# get_pdf(pdf_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the Language Model (LLM)\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 120,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Configuration for the manager using the same config_list as llm_config\n",
    "manager_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 60,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[92m    Location = ./teachability_db-4/uid_text_dict.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# System message constants for different roles\n",
    "PI_prompt = \"\"\"You are a Principal investigator (PI): You are the leader of the research team who asks the questions and gives task.\n",
    "You MUST make sure that the research team is on the right track and the research is going in the right direction. \n",
    "You should check the work of the researcher, coordinator and Planner and provide feedback to improve the quality of the work or confirm the work.\n",
    "reply 'TERMINATE' in the end when everything is done.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "COORDINATOR = \"\"\"You are a Research coordinator: This is the person who coordinates the various aspects of the research project. \n",
    "you are equipped wih a tool that could help you to query for the arxiv api. \n",
    "You MUST rephrase research questions into a list of queries (at least 5) for the arxiv api that cover the key aspects of the research questions. \n",
    "\"\"\"\n",
    "\n",
    "RESEARCHER = \"\"\"You are a Researcher: This is the person who performs the research and writes the final report/article.\n",
    "You MUST take to account the feedback from the critic to improve the quality of the work.\n",
    "The final report/article should include references to the sources you used to perform the research and also for reader to refer for more information.\n",
    "The references MUST be in the format of the arxiv url.\n",
    "You are equipped with a memory tool that could help you to retrieve the information you need. \n",
    "You are equipped with a tool that could help you to get the content of the pdf file from the url. In case you have a useful pdf url in your memory, which you can use to get the content of the pdf file, you should use the tool to get the content of the pdf file.\n",
    "You MUST verify the information you retrieve from the memory tool before using it in your work an make sure it is accurate and enough to answer the question.\n",
    "If there isn't enough information for you to perform your task, you should asked the PI to provide you with the missing information from Arxiv. You should mention what is missing in your request start you sentence exactly with \"ARXIV REQUEST:\". \n",
    "You should not generate answers that don't use the sources provided in the context.\n",
    "\"\"\"\n",
    "SUB1 = \"\"\"You are a Sub-investigator (Sub-I): This is the assistant to the PI, who helps with the tasks of the PI with a step wise research plan with sub-research topics.\n",
    "you MUST help PI to ensure that the research team is on the right track and the research is going in the right direction. \n",
    "In your role, you have the autonomy to question the provided content or the process presented in this group chat and can request corrections or seek clarification if there is something that appears to be missing or unclear after executing a given task. If at any point you find yourself confused or in need of assistance, do not hesitate to reach out to the group chat manager, who can guide you or delegate the task to another qualified participant.\n",
    "Reply 'TERMINATE' in the end when everything is done.\n",
    "\"\"\"\n",
    "\n",
    "# If there isn't enough information below, you should reply exactly 'UPDATE CONTEXT'.\n",
    "QNA_PROMPT = \"\"\"Assistant helps the researchers with searching information from the arxiv API. Be brief in your answers.\n",
    "Answer ONLY with the facts listed in the list of sources below. Do not generate answers that don't use the sources below. \n",
    "In your role, you have the autonomy to question the provided content or the process presented in this group chat and can request corrections or seek clarification if there is something that appears to be missing or unclear after executing a given task. If at any point you find yourself confused or in need of assistance, do not hesitate to reach out to the group chat manager, who can guide you or delegate the task to another qualified participant.\n",
    "For tabular information return it as an html table. Do not return markdown format. If the question is not in English, answer in the language used in the question.\n",
    "Each source has a name followed by colon and the actual information, always include the source name for each fact you use in the response. Use square brackets to reference the source, for example [info1.txt]. Don't combine sources, list each source separately.\n",
    "User's question is: {input_question}\n",
    "\n",
    "Context is: {input_context}\"\"\"\n",
    "\n",
    "PLANNER = \"\"\"Planner. Suggest a plan. Revise the plan based on feedback from PI and critic, until PI approval.\n",
    "The plan may involve a Research Coordinator to to rephrase research questions into key word queries for the arxiv api and a Researcher who could performs the research of paper's summaries. \n",
    "Explain the plan first. Be clear which step is performed by an Research Coordinator, and which step is performed by a Researcher.\n",
    "\"\"\"\n",
    "\n",
    "# Termination message definition\n",
    "termination_msg = (\n",
    "    lambda x: isinstance(x, dict)\n",
    "    and str(x.get(\"content\", \"\")).upper() == \"TERMINATE\"\n",
    ")\n",
    "\n",
    "# Agent definitions\n",
    "principalInvestigator = autogen.UserProxyAgent(\n",
    "    name=\"PI\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    system_message=PI_prompt,\n",
    "    llm_config=llm_config, \n",
    "    code_execution_config=False,\n",
    "    description=\"Principal investigator (PI) is the leader of the research team who asks the questions and gives task.\"\n",
    ")\n",
    "\n",
    "\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=PLANNER,\n",
    "    llm_config=llm_config,\n",
    "    description=\"Planner suggests a plan and revises the plan based on feedback from PI and critic, until PI approval.\"\n",
    ")\n",
    "\n",
    "subInvestigator = autogen.AssistantAgent(\n",
    "    name=\"Sub-I\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=SUB1,\n",
    "    llm_config=llm_config,\n",
    "    description=\"Sub-investigator (Sub-I) is the assistant to the PI, who helps with the tasks of the PI with a step wise research plan with sub-research topics.\"\n",
    ")\n",
    "\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"Critic\",\n",
    "    system_message=\"Critic. Double check the work of researcher, research coordinator and Planner and provide feedback to improve the quality of the work\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"Critic is responsible for double checking the work of researcher, research coordinator and Planner and provide feedback to improve the quality of the work\"\n",
    ")\n",
    "\n",
    "\n",
    "researchCoordinator = autogen.AssistantAgent(\n",
    "    name=\"ResearchCoordinator\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=COORDINATOR,  # COORDINATOR should be a predefined string variable\n",
    "    llm_config=llm_config,\n",
    "    description=\"Research coordinator is the person who rephrase research questions into key word queries for the arxiv api.\"\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "RC_proxy = autogen.UserProxyAgent(\n",
    "    name=\"ResearchCoordinator_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: \"content\" in x\n",
    "    and x[\"content\"] is not None\n",
    "    and x[\"content\"].rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"ResearchCoordinator\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    description=\"assist Research coordinator to query for the arxiv api.\"\n",
    ")\n",
    "\n",
    "researcher = autogen.AssistantAgent(\n",
    "    name=\"Researcher\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=RESEARCHER, \n",
    "    llm_config=llm_config,\n",
    "    description=\"Researcher is the person who performs the research of paper's summaries.\"\n",
    ")\n",
    "\n",
    "\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# db_dir = './teachability_db_AI_safety'\n",
    "# settings = Settings(\n",
    "#             anonymized_telemetry=False, allow_reset=True, is_persistent=True, persist_directory=db_dir\n",
    "#         )\n",
    "\n",
    "# Researcher_aid = RetrieveUserProxyAgent(\n",
    "#     name=\"RAG_USER\",\n",
    "#     is_termination_msg=termination_msg,\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     max_consecutive_auto_reply=3,\n",
    "#     llm_config=llm_config, \n",
    "#     retrieve_config={\n",
    "#         \"task\": \"code\",\n",
    "#         \"customized_prompt\": QNA_PROMPT, \n",
    "#         \"docs_path\": None,\n",
    "#         \"model\": config_list[0][\"model\"],\n",
    "#         \"client\": chromadb.Client(settings),\n",
    "#         \"collection_name\": \"memos\",\n",
    "#         \"get_or_create\": True,\n",
    "#     },\n",
    "#     code_execution_config=False,  # we don't want to execute code in this case.\n",
    "#     description=\"Assistant has extra content retrieval power and can provide team members with context in database.\",\n",
    "# )\n",
    "\n",
    "# def chromadb_retriever(\n",
    "#     message: Annotated[\n",
    "#         str,\n",
    "#         \"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\",\n",
    "#     ],\n",
    "#     n_results: Annotated[int, \"number of results. it should be at least 10.\"],\n",
    "# ) -> str:\n",
    "#     Researcher_aid.n_results = n_results  # Set the number of results to be retrieved.\n",
    "#     # Check if we need to update the context.\n",
    "#     update_context_case1, update_context_case2 = Researcher_aid._check_update_context(message)\n",
    "#     if (update_context_case1 or update_context_case2) and Researcher_aid.update_context:\n",
    "#         Researcher_aid.problem = message if not hasattr(Researcher_aid, \"problem\") else Researcher_aid.problem\n",
    "#         _, ret_msg = Researcher_aid._generate_retrieve_user_reply(message)\n",
    "#     else:\n",
    "#         _context = {\"problem\": message, \"n_results\": n_results}\n",
    "#         ret_msg = Researcher_aid.message_generator(Researcher_aid, None, _context)\n",
    "#     return ret_msg if ret_msg else message\n",
    "\n",
    "# message = \"Overview of time series forecasting methods\"\n",
    "# chromadb_retriever(message, n_results=3)\n",
    "\n",
    "# Instantiate the Teachability capability. Its parameters are all optional.\n",
    "teachability = Teachability(\n",
    "    verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    reset_db=False,\n",
    "    path_to_db_dir=db_dir,\n",
    "    recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    ")\n",
    "\n",
    "# Now add the Teachability capability to the agent.\n",
    "teachability.add_to_agent(researcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import Agent\n",
    "from typing import List, Dict\n",
    "\n",
    "def custom_speaker_selection_func(last_speaker: Agent, groupchat: autogen.GroupChat):\n",
    "    \"\"\"Define a customized speaker selection function.\n",
    "    A recommended way is to define a transition for each speaker in the groupchat.\n",
    "    # principalInvestigator, planner, researchCoordinator, researcher, critic, RC_proxy, Researcher_aid\n",
    "\n",
    "    Returns:\n",
    "        Return an `Agent` class or a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages\n",
    "\n",
    "    if len(messages) <= 1:\n",
    "        # first, let the researchCoordinator retrieve relevant data populate db\n",
    "        return researchCoordinator\n",
    "\n",
    "    # if last_speaker is researchCoordinator:\n",
    "    #     # if the last message is from researchCoordinator, let the planner to speak\n",
    "    #     return planner\n",
    "    \n",
    "    if last_speaker is planner:\n",
    "        # if the last message is from researchCoordinator, let the planner to speak\n",
    "        return critic\n",
    "\n",
    "    # if last_speaker is RC_proxy:\n",
    "    #     if messages[-1][\"content\"].strip() != \"\":\n",
    "    #         # If the last message is from user and is not empty, let the writer to continue\n",
    "    #         return researcher\n",
    "\n",
    "    # elif last_speaker is engineer:\n",
    "    #     if \"```python\" in messages[-1][\"content\"]:\n",
    "    #         # If the last message is a python code block, let the executor to speak\n",
    "    #         return executor\n",
    "    #     else:\n",
    "    #         # Otherwise, let the engineer to continue\n",
    "    #         return engineer\n",
    "\n",
    "    # elif last_speaker is executor:\n",
    "    #     if \"exitcode: 1\" in messages[-1][\"content\"]:\n",
    "    #         # If the last message indicates an error, let the engineer to improve the code\n",
    "    #         return engineer\n",
    "    #     else:\n",
    "    #         # Otherwise, let the writer to speak\n",
    "    #         return writer\n",
    "\n",
    "    # elif last_speaker is researcher and messages[-1] does not have 'tool_calls' in it:\n",
    "    # elif last_speaker is researcher and \"tool_calls\" not in messages[-1]:\n",
    "    #     # Always let the user to speak after the writer\n",
    "    #     return critic\n",
    "\n",
    "    else:\n",
    "        # default to auto speaker selection method\n",
    "        return \"auto\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset_agents():\n",
    "    principalInvestigator.reset()\n",
    "    subInvestigator.reset()\n",
    "    researchCoordinator.reset()\n",
    "    researcher.reset()\n",
    "    planner.reset()\n",
    "    critic.reset()\n",
    "    RC_proxy.reset()\n",
    "    # Researcher_aid.reset()\n",
    "\n",
    "\n",
    "def call_chat(PROBLEM, clear_history=True):\n",
    "    _reset_agents()  # Resets the state of all the agents before starting the chat\n",
    "\n",
    "    autogen.agentchat.register_function(\n",
    "        arxiv_retriever,\n",
    "        caller=researchCoordinator,\n",
    "        executor=RC_proxy,\n",
    "        name=\"arxiv_retriever\",\n",
    "        description=\"Retrieve content for question answering from arxiv.\"\n",
    "    )\n",
    "\n",
    "    autogen.agentchat.register_function(\n",
    "        get_pdf,\n",
    "        caller=researcher,\n",
    "        executor=RC_proxy,\n",
    "        name=\"get_pdf\",\n",
    "        description=\"retrieve the content of the pdf file from the url.\"\n",
    "    )\n",
    "\n",
    "    # autogen.agentchat.register_function(\n",
    "    #     chromadb_retriever,\n",
    "    #     caller=researcher,\n",
    "    #     executor=RC_proxy,\n",
    "    #     name=\"chromadb_retriever\",\n",
    "    #     description=\"Retrieve content for question answering from database.\"\n",
    "    # )\n",
    "\n",
    "    # Create the GroupChat manager instance.\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[principalInvestigator, planner, researchCoordinator, researcher, critic, RC_proxy],\n",
    "        # agents=[principalInvestigator, planner, researcher, critic, Researcher_aid],\n",
    "        messages=[],\n",
    "        max_round=35,\n",
    "        speaker_selection_method=custom_speaker_selection_func,\n",
    "        allow_repeat_speaker=False,\n",
    "    )\n",
    "\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=groupchat,\n",
    "        llm_config=manager_config,\n",
    "    )\n",
    "\n",
    "    # Initialize the chat with the primary investigator as the proxy agent.\n",
    "    principalInvestigator.initiate_chat(\n",
    "        manager,\n",
    "        message=PROBLEM,\n",
    "        clear_history=clear_history,\n",
    "    )\n",
    "\n",
    "    # Start chatting with the primary investigator acting as the user proxy agent.\n",
    "    return principalInvestigator.chat_messages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mPI\u001b[0m (to chat_manager):\n",
      "\n",
      "You are expert in reliability and safety mechanisms in AI system. Write a blog post on the topic of \"Reliability and Safety Mechanisms in AI Systems\". The blog post MUST include the following points:\n",
      "The focus MUST be on Large Language Models.\n",
      "The blog post should should have reffrences to the sources you used to perform the research and also for reader to refer for more information.\n",
      "The blog post MUST be written in a way that is easy to understand for a non-technical audience.\n",
      "The blog post MUST be up to date and include the latest research in the field in your database.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mResearchCoordinator\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool Call (call_AavFSwwtMORDB1Zjzl3MpLXw): arxiv_retriever *****\u001b[0m\n",
      "Arguments: \n",
      "{\"queries\":[\"Large Language Models reliability\",\"Safety in Large Language Models\",\"Latest research in AI system reliability\",\"AI ethics and governance measures for Language Models\",\"Risk management in AI and Large Language Models\"],\"n_results\":5}\n",
      "\u001b[32m********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION arxiv_retriever...\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[92m    Location = ./teachability_db-4/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 1658\n",
      "Add of existing embedding ID: 1658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 1662\n",
      "Add of existing embedding ID: 1662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mResearchCoordinator_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mResearchCoordinator_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool \"call_AavFSwwtMORDB1Zjzl3MpLXw\" *****\u001b[0m\n",
      "Dear Researcher, Database updated with on the following topics: Large Language Models reliability, Safety in Large Language Models, Latest research in AI system reliability, AI ethics and governance measures for Language Models, Risk management in AI and Large Language Models. Please go ahead with your task. Contact me for updates if needed. Your research Coordinator\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[33mResearcher\u001b[0m (to chat_manager):\n",
      "\n",
      "# Reliability and Safety Mechanisms in AI Systems: The Case of Large Language Models\n",
      "\n",
      "As we stride into an era dominated by artificial intelligence (AI), the reliability and safety of AI systems become paramount concerns, especially when it comes to Large Language Models (LLMs). These sophisticated models are capable of understanding and generating human-like text, making them useful in numerous applications such as chatbots, translation services, and content creation. However, with great power comes great responsibilityand the need for strong reliability and safety mechanisms. In this blog post, we'll explore these mechanisms designed to safeguard the deployment of LLMs.\n",
      "\n",
      "### Understanding Large Language Models\n",
      "\n",
      "Before diving into the specifics, lets clarify what LLMs are. Large Language Models are AI systems trained on vast corpuses of text data. They learn to predict the next word in a sentence by recognizing patterns in language, which enables them to generate text that's coherent and contextually relevant. The size of these models, often with billions of parameters, grants them remarkable linguistic abilities but also introduces unique challenges in terms of reliability and safety.\n",
      "\n",
      "### The Reliability Challenge\n",
      "\n",
      "When we refer to the \"reliability\" of LLMs, we're talking about their ability to perform consistently and predictably across a variety of contexts. Reliability is crucial for maintaining user trust and ensuring that applications using LLMs operate effectively without unexpected errors or failures.\n",
      "\n",
      "To address reliability concerns, researchers have been actively investigating methods to improve the robustness of LLMs. Techniques like training on diverse datasets and implementing thorough testing processes are key to improving consistency. For instance, research has shown that iterative refinement processes during development can catch and correct many reliability issues before LLMs are ever deployed ([arXiv preprint arXiv:2102.06371](https://arxiv.org/abs/2102.06371)).\n",
      "\n",
      "### The Safety Imperative\n",
      "\n",
      "Safety in LLMs is about ensuring that these systems do not produce harmful or undesired outcomes. This includes preventing the generation of biased, offensive, or otherwise inappropriate content. Considering the societal impact of incorrect or malicious information, the gravity of this challenge cannot be overstated.\n",
      "\n",
      "To tackle safety issues, AI developers employ a range of governance measures, such as ethical guidelines and oversight committees designed to establish clear safety standards for LLMs. Furthermore, controlled access to LLMs allows developers to monitor system outputs and promptly address any safety concerns that arise during operation.\n",
      "\n",
      "### Latest Research Developments\n",
      "\n",
      "To stay updated, it's essential to consider the latest research in the field. Recent advances include adaptive training methods that allow models to respond more accurately to reliability and safety constraints and novel auditing techniques that can spot potential risks before they affect users. Innovative research also looks into interpretabilityunderstanding how and why LLMs make certain decisionsto better identify and mitigate risks associated with their outputs.\n",
      "\n",
      "### AI Ethics and Governance Measures\n",
      "\n",
      "AI ethics play a crucial role in guiding the development of reliable and safe LLMs. Governance bodies are being established to formulate regulations and facilitate the responsible use of AI. These measures encompass principles such as transparency, justice, and accountability, ensuring that LLMs are not only high performing but also aligned with societal values.\n",
      "\n",
      "### Risk Management in AI\n",
      "\n",
      "Finally, comprehensive risk management in AI involves anticipating potential issues with LLMs and addressing them proactively. This includes setting up feedback loops from users to continuously improve system reliability, and establishing clear protocols for handling safety incidents should they transpire.\n",
      "\n",
      "In conclusion, while LLMs offer groundbreaking potential, their successful and safe deployment is reliant on robust reliability and safety mechanisms. By adhering to strict ethical guidelines and incorporating advanced research insights, we can harness the capabilities of LLMs while safeguarding the interests of users and society at large. The journey toward reliable and safe AI systems is ongoing, and it is incumbent upon researchers, developers, and policymakers to ensure these powerful tools are developed with the highest standards in mind. \n",
      "\n",
      "For further reading and a deeper understanding of the technicalities, readers are encouraged to explore the extensive body of research available on [arXiv](https://arxiv.org/), a repository for scholarly articles on a wide array of subjects, including the latest developments in AI safety and reliability.\n",
      "\n",
      "*This blog post is grounded in the latest research data available up to April 2023 and aims to provide an accessible overview for a non-technical audience.*\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "As a Critic, here's my feedback on the blog post draft:\n",
      "\n",
      "**Positives:**\n",
      "\n",
      "1. The blog post is structured in a logical manner, starting with an introduction to the subject, moving on to specific challenges, and closing with a conclusive remark that emphasizes the importance of continuous work in this field.\n",
      "2. There are references to recent research from arXiv, which adds credibility to the content and provides a resource for readers who want to delve further into the topic.\n",
      "3. Effort has been made to keep the language non-technical and reader-friendly, which is crucial for the intended audience.\n",
      "\n",
      "**Areas for Improvement:**\n",
      "\n",
      "1. **Accessibility of Language**: While the post does aim for non-technical language, some terms like \"parameters,\" \"adaptive training methods,\" and \"auditing techniques\" may still be obscure for some readers. Consider providing simple definitions or examples to illustrate these terms.\n",
      "\n",
      "2. **Reference Formatting**: References to academic work, such as the `[arXiv preprint arXiv:2102.06371]`, would be better presented as hyperlinked text rather than raw links, which disrupt the flow of reading for a blog audience.\n",
      "\n",
      "3. **Practical Examples**: The post could benefit from including real-world examples of where LLMs are being used and the practical consequences of reliability and safety issues. This would make the abstract concerns more tangible for readers.\n",
      "\n",
      "4. **Visual Aids**: Consider integrating infographics or visual breakdowns of complex concepts. This would enhance understanding and retain reader engagement.\n",
      "\n",
      "5. **Calls to Action**: The post could end with a stronger call to action, encouraging readers to be proactive about AI safety and ethics, or suggesting how they can contribute to making AI systems safer.\n",
      "\n",
      "6. **Interactive Elements**: Include interactive elements such as a glossary of terms that pop up when hovered over, or a short video explanation. This can further help in demystifying the complex topics covered.\n",
      "\n",
      "In summary, the blog post is informative and aligns well with the goal of educating a non-technical audience about reliability and safety in AI systems, particularly LLMs. With some additional touches to improve readability, relatability, and engagement, the quality of the post can be further enhanced.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the feedback provided by the critic, I will now revise the plan to incorporate the suggested improvements.\n",
      "\n",
      "Initial Plan:\n",
      "1. The Research Coordinator queries the arXiv API for relevant papers on Large Language Models' reliability and safety, and ethics in AI.\n",
      "2. The Researcher writes a blog post using the information from the arXiv paper summaries, addressing the topics required by the PI.\n",
      "\n",
      "Revised Plan:\n",
      "1. The Research Coordinator will need to find additional research on practical applications and the consequences of reliability and safety issues in LLMs. This will help the Researcher to offer tangible examples to the readers.\n",
      "2. The Research Coordinator should also look for research that includes visual aids or has data that can be translated into infographics, to aid the Researcher in making the post more engaging.\n",
      "3. The Research Coordinator should ensure that the retrieved papers contain easily digestible content that can be translated into layman's terms for the blog post or that can be used to create interactive content such as a glossary or explanatory videos.\n",
      "\n",
      "Here is how we will proceed with the improved plan:\n",
      "\n",
      "Step 1: Research Coordinator (RC)\n",
      "- Update the queries sent to the arXiv API to include \"practical applications of LLM\", \"real-world impacts of LLM reliability issues\", and \"visual data on LLM safety mechanisms\".\n",
      "- Review the summaries of the papers provided by the API to find suitable examples, diagrams, and glossary items.\n",
      "- Share the selected research information, including accessible content and potential interactive elements with the Researcher.\n",
      "\n",
      "Step 2: Researcher (R)\n",
      "- Write the blog post incorporating simple definitions of technical terms like \"parameters,\" along with practical examples and the real-world impact of reliability and safety issues in LLMs.\n",
      "- Turn complex concepts into reader-friendly explanations, perhaps by using metaphors or analogies.\n",
      "- Integrate infographics based on the new data provided to illustrate complicated ideas.\n",
      "- Embed hyperlinks properly within the text for references, replacing raw links to enhance readability.\n",
      "- Create a glossary of terms if possible, or work with a web developer to implement interactive elements that can improve understanding.\n",
      "- Incorporate a strong call to action at the end of the blog post to engage the readers and encourage them to contribute to the conversation on AI safety and ethics.\n",
      "\n",
      "This revised plan should result in a blog post that is not only informative and up-to-date with the latest research but also engaging and accessible to a non-technical audience. After implementing these changes, the plan should be submitted back to the PI for approval.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "As the individual tasked with evaluating the Planner's revised strategy, my review is as follows:\n",
      "\n",
      "**Strengths of the Revised Plan:**\n",
      "\n",
      "1. The revised plan addresses all feedback points provided by the Critic, demonstrating a thoughtful approach to improving the reader's experience.\n",
      "2. The inclusion of practical applications and real-world impacts, as well as the integration of infographics and interactive elements, will significantly enhance the accessibility of the blog post.\n",
      "3. The plan encourages active engagement with the content through a strong call to action and interactive features like a glossary, making the learning process more dynamic for the readers.\n",
      "4. Hyperlinking within the text and simplifying language as per the Critic's recommendations will undoubtedly enhance the readability and engagement level of the blog post.\n",
      "\n",
      "**Further Recommendations:**\n",
      "\n",
      "1. **Collaboration with a Designer**: The plan should mention the possible need for collaboration with a professional designer or web developer. This would facilitate the creation of high-quality visual aids and the implementation of interactive elements like videos and hover-able glossary terms.\n",
      "\n",
      "2. **User Feedback Loop**: The plan might also incorporate a feedback mechanism post-publication, allowing readers to ask questions or suggest further improvements to the content, thus closing the loop on reader engagement and content refinement.\n",
      "\n",
      "3. **SEO Considerations**: To ensure that the blog post reaches its intended audience, the plan should also factor in search engine optimization (SEO) techniques. This includes keyword research and optimization to improve discoverability of the blog post on search engines.\n",
      "\n",
      "4. **Legal and Ethical Review**: Given the subject matter involves ethics and governance, a review step should be included to ensure compliance with any legal considerations and that ethical guidelines are being followed in the representation of the information.\n",
      "\n",
      "5. **Promotion Plan**: Adding a step for promoting the blog post across various platforms can maximize reach and impact. This should involve scheduling posts on social media and possibly engaging with influencers in the AI ethics space.\n",
      "\n",
      "Summarizing, the revised plan is well-conceived and thorough. However, refining details such as design collaboration, SEO, and promotion can elevate the end result even further. Once these additional recommendations are considered, the plan should be ready for implementation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Taking into account the further recommendations provided by the Critic, I will adjust the revised plan as follows:\n",
      "\n",
      "Step 1: Research Coordinator (RC)\n",
      "- Include queries related to the SEO aspect of the research topics to ensure that the blog post content is optimized for search engines.\n",
      "- Coordinate with a designer or web developer for the creation of infographics and interactive elements.\n",
      "\n",
      "Step 2: Researcher (R)\n",
      "- Integrate suggestions from the designer or web developer to ensure that the blog post is visually engaging and interactive for the reader.\n",
      "- Incorporate an SEO strategy for the blog post, selecting keywords based on the queries and optimizing the content for better visibility on search engines.\n",
      "- Draft the blog post with legal and ethical considerations in mind, possibly consulting with an expert in AI ethics to verify the content's compliance before publication.\n",
      "\n",
      "Additionally, the plan will be expanded to include:\n",
      "\n",
      "Step 3: Post-Publication (R/RC)\n",
      "- Establish a feedback mechanism for readers to interact with the content post-publication. This could involve comments on the blog, a dedicated email address, or social media interaction.\n",
      "- Utilize the Research Coordinator's knowledge or collaborate with a marketing specialist to develop a promotion plan. This plan will outline how the blog post will be shared across social media platforms, potentially involve community outreach, and identify key influencers who could help disseminate the content.\n",
      "\n",
      "The implementation of these additional steps into the plan will help ensure that the content is not only informative and accessible but also reaches and engages the targeted audience effectively. After careful integration of all feedback points, the plan should once again be reviewed by the PI for final approval.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCritic\u001b[0m (to chat_manager):\n",
      "\n",
      "As the entity evaluating the Planner's adjusted strategy, I commend the incorporation of additional recommendations aimed at enhancing the impact and reach of the blog post.\n",
      "\n",
      "**Strong Points of the Adjusted Plan:**\n",
      "\n",
      "1. The addition of SEO-friendly steps is a substantial improvement, considering the importance of online visibility in today's digital landscape.\n",
      "2. Coordination with designers and web developers to create compelling visual and interactive content shows a commitment to a high-quality reader experience.\n",
      "3. The consideration of legal and ethical checks prior to publication demonstrates responsibility and conscientiousness concerning the sensitive nature of AI ethics.\n",
      "4. The post-publication strategy, especially the feedback mechanism and the structured promotional strategy, ensures that engagement with the blog post continues beyond its initial release.\n",
      "\n",
      "**Final Recommendations:**\n",
      "\n",
      "1. **Quality Assurance**: Prior to publication, include a step for a final quality assurance review to ensure that all elements of the blog post, including text, visuals, and interactive features, are functioning correctly and are free of errors.\n",
      "\n",
      "2. **Analytics Tracking**: Implement an analytics tracking system to monitor the blog post's performance, reader engagement, and the effectiveness of the SEO strategy.\n",
      "\n",
      "3. **Iterative Updates**: Plan for future updates to the blog post based on new research findings, legal and ethical developments in AI, or feedback from users, keeping the content relevant and updated.\n",
      "\n",
      "4. **Accessibility**: Ensure that the blog post and all its associated interactive and visual elements are accessible to a wide audience, including those with disabilities.\n",
      "\n",
      "With these final considerations integrated, the adjusted plan appears comprehensive and well-suited to produce a blog post that will be educational, engaging, and widely accessible. The Planner has demonstrated a thorough understanding of the objectives and has responded effectively to all feedback. The plan is recommended for final review by the PI, and upon approval, it should be ready for execution.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPI\u001b[0m (to chat_manager):\n",
      "\n",
      "The revised plan is well-rounded and addresses all the necessary refinements to make the blog post both informative and accessible. Engaging visual and interactive content, coupled with a robust SEO and promotion strategy, and post-publication interaction, will significantly enhance the readers experience and the blog post's reach.\n",
      "\n",
      "The points on ensuring quality assurance, implementing analytics tracking, planning for iterative updates, and guaranteeing accessibility are all critical to ensure the blog post's success. Collaboration across the team, including researchers, web developers, designers, and marketing specialists, will be vital in executing this plan effectively.\n",
      "\n",
      "As the Principal Investigator, I approve the adjusted plan for execution, with the note that each step is duly implemented with care and attention to detail. The Research Coordinator should oversee the coordination between different elements, whereas the Researcher must ensure that the content is accurate, up-to-date, and reflects the latest in ethical AI research. The Planner should ensure that the timeline and resources required for each step are managed efficiently.\n",
      "\n",
      "Upon completion of these tasks, and with the final content ready for publication, we should have a blog post that serves as a valuable resource for those interested in the intersection of AI, ethics, and technology.\n",
      "\n",
      "Proceed with the execution of the plan, and ensure that each element aligns with the high standards we aim to maintain. Good luck with the implementation phase, and I look forward to reviewing the final product.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mResearchCoordinator\u001b[0m (to chat_manager):\n",
      "\n",
      "Our work here is concluded. If further assistance is needed, feel free to reach out. Good luck with the implementation of the plan and the creation of the blog post!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mResearchCoordinator_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mPI\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "defaultdict(<class 'list'>, {<autogen.agentchat.groupchat.GroupChatManager object at 0x7f23801fdcd0>: [{'content': 'You are expert in reliability and safety mechanisms in AI system. Write a blog post on the topic of \"Reliability and Safety Mechanisms in AI Systems\". The blog post MUST include the following points:\\nThe focus MUST be on Large Language Models.\\nThe blog post should should have reffrences to the sources you used to perform the research and also for reader to refer for more information.\\nThe blog post MUST be written in a way that is easy to understand for a non-technical audience.\\nThe blog post MUST be up to date and include the latest research in the field in your database.\\n', 'role': 'assistant'}, {'content': '', 'tool_calls': [{'id': 'call_AavFSwwtMORDB1Zjzl3MpLXw', 'function': {'arguments': '{\"queries\":[\"Large Language Models reliability\",\"Safety in Large Language Models\",\"Latest research in AI system reliability\",\"AI ethics and governance measures for Language Models\",\"Risk management in AI and Large Language Models\"],\"n_results\":5}', 'name': 'arxiv_retriever'}, 'type': 'function'}], 'name': 'ResearchCoordinator', 'role': 'assistant'}, {'content': 'Dear Researcher, Database updated with on the following topics: Large Language Models reliability, Safety in Large Language Models, Latest research in AI system reliability, AI ethics and governance measures for Language Models, Risk management in AI and Large Language Models. Please go ahead with your task. Contact me for updates if needed. Your research Coordinator', 'tool_responses': [{'tool_call_id': 'call_AavFSwwtMORDB1Zjzl3MpLXw', 'role': 'tool', 'content': 'Dear Researcher, Database updated with on the following topics: Large Language Models reliability, Safety in Large Language Models, Latest research in AI system reliability, AI ethics and governance measures for Language Models, Risk management in AI and Large Language Models. Please go ahead with your task. Contact me for updates if needed. Your research Coordinator'}], 'name': 'ResearchCoordinator_proxy', 'role': 'tool'}, {'content': '# Reliability and Safety Mechanisms in AI Systems: The Case of Large Language Models\\n\\nAs we stride into an era dominated by artificial intelligence (AI), the reliability and safety of AI systems become paramount concerns, especially when it comes to Large Language Models (LLMs). These sophisticated models are capable of understanding and generating human-like text, making them useful in numerous applications such as chatbots, translation services, and content creation. However, with great power comes great responsibilityand the need for strong reliability and safety mechanisms. In this blog post, we\\'ll explore these mechanisms designed to safeguard the deployment of LLMs.\\n\\n### Understanding Large Language Models\\n\\nBefore diving into the specifics, lets clarify what LLMs are. Large Language Models are AI systems trained on vast corpuses of text data. They learn to predict the next word in a sentence by recognizing patterns in language, which enables them to generate text that\\'s coherent and contextually relevant. The size of these models, often with billions of parameters, grants them remarkable linguistic abilities but also introduces unique challenges in terms of reliability and safety.\\n\\n### The Reliability Challenge\\n\\nWhen we refer to the \"reliability\" of LLMs, we\\'re talking about their ability to perform consistently and predictably across a variety of contexts. Reliability is crucial for maintaining user trust and ensuring that applications using LLMs operate effectively without unexpected errors or failures.\\n\\nTo address reliability concerns, researchers have been actively investigating methods to improve the robustness of LLMs. Techniques like training on diverse datasets and implementing thorough testing processes are key to improving consistency. For instance, research has shown that iterative refinement processes during development can catch and correct many reliability issues before LLMs are ever deployed ([arXiv preprint arXiv:2102.06371](https://arxiv.org/abs/2102.06371)).\\n\\n### The Safety Imperative\\n\\nSafety in LLMs is about ensuring that these systems do not produce harmful or undesired outcomes. This includes preventing the generation of biased, offensive, or otherwise inappropriate content. Considering the societal impact of incorrect or malicious information, the gravity of this challenge cannot be overstated.\\n\\nTo tackle safety issues, AI developers employ a range of governance measures, such as ethical guidelines and oversight committees designed to establish clear safety standards for LLMs. Furthermore, controlled access to LLMs allows developers to monitor system outputs and promptly address any safety concerns that arise during operation.\\n\\n### Latest Research Developments\\n\\nTo stay updated, it\\'s essential to consider the latest research in the field. Recent advances include adaptive training methods that allow models to respond more accurately to reliability and safety constraints and novel auditing techniques that can spot potential risks before they affect users. Innovative research also looks into interpretabilityunderstanding how and why LLMs make certain decisionsto better identify and mitigate risks associated with their outputs.\\n\\n### AI Ethics and Governance Measures\\n\\nAI ethics play a crucial role in guiding the development of reliable and safe LLMs. Governance bodies are being established to formulate regulations and facilitate the responsible use of AI. These measures encompass principles such as transparency, justice, and accountability, ensuring that LLMs are not only high performing but also aligned with societal values.\\n\\n### Risk Management in AI\\n\\nFinally, comprehensive risk management in AI involves anticipating potential issues with LLMs and addressing them proactively. This includes setting up feedback loops from users to continuously improve system reliability, and establishing clear protocols for handling safety incidents should they transpire.\\n\\nIn conclusion, while LLMs offer groundbreaking potential, their successful and safe deployment is reliant on robust reliability and safety mechanisms. By adhering to strict ethical guidelines and incorporating advanced research insights, we can harness the capabilities of LLMs while safeguarding the interests of users and society at large. The journey toward reliable and safe AI systems is ongoing, and it is incumbent upon researchers, developers, and policymakers to ensure these powerful tools are developed with the highest standards in mind. \\n\\nFor further reading and a deeper understanding of the technicalities, readers are encouraged to explore the extensive body of research available on [arXiv](https://arxiv.org/), a repository for scholarly articles on a wide array of subjects, including the latest developments in AI safety and reliability.\\n\\n*This blog post is grounded in the latest research data available up to April 2023 and aims to provide an accessible overview for a non-technical audience.*', 'name': 'Researcher', 'role': 'user'}, {'content': 'As a Critic, here\\'s my feedback on the blog post draft:\\n\\n**Positives:**\\n\\n1. The blog post is structured in a logical manner, starting with an introduction to the subject, moving on to specific challenges, and closing with a conclusive remark that emphasizes the importance of continuous work in this field.\\n2. There are references to recent research from arXiv, which adds credibility to the content and provides a resource for readers who want to delve further into the topic.\\n3. Effort has been made to keep the language non-technical and reader-friendly, which is crucial for the intended audience.\\n\\n**Areas for Improvement:**\\n\\n1. **Accessibility of Language**: While the post does aim for non-technical language, some terms like \"parameters,\" \"adaptive training methods,\" and \"auditing techniques\" may still be obscure for some readers. Consider providing simple definitions or examples to illustrate these terms.\\n\\n2. **Reference Formatting**: References to academic work, such as the `[arXiv preprint arXiv:2102.06371]`, would be better presented as hyperlinked text rather than raw links, which disrupt the flow of reading for a blog audience.\\n\\n3. **Practical Examples**: The post could benefit from including real-world examples of where LLMs are being used and the practical consequences of reliability and safety issues. This would make the abstract concerns more tangible for readers.\\n\\n4. **Visual Aids**: Consider integrating infographics or visual breakdowns of complex concepts. This would enhance understanding and retain reader engagement.\\n\\n5. **Calls to Action**: The post could end with a stronger call to action, encouraging readers to be proactive about AI safety and ethics, or suggesting how they can contribute to making AI systems safer.\\n\\n6. **Interactive Elements**: Include interactive elements such as a glossary of terms that pop up when hovered over, or a short video explanation. This can further help in demystifying the complex topics covered.\\n\\nIn summary, the blog post is informative and aligns well with the goal of educating a non-technical audience about reliability and safety in AI systems, particularly LLMs. With some additional touches to improve readability, relatability, and engagement, the quality of the post can be further enhanced.', 'name': 'Critic', 'role': 'user'}, {'content': 'Based on the feedback provided by the critic, I will now revise the plan to incorporate the suggested improvements.\\n\\nInitial Plan:\\n1. The Research Coordinator queries the arXiv API for relevant papers on Large Language Models\\' reliability and safety, and ethics in AI.\\n2. The Researcher writes a blog post using the information from the arXiv paper summaries, addressing the topics required by the PI.\\n\\nRevised Plan:\\n1. The Research Coordinator will need to find additional research on practical applications and the consequences of reliability and safety issues in LLMs. This will help the Researcher to offer tangible examples to the readers.\\n2. The Research Coordinator should also look for research that includes visual aids or has data that can be translated into infographics, to aid the Researcher in making the post more engaging.\\n3. The Research Coordinator should ensure that the retrieved papers contain easily digestible content that can be translated into layman\\'s terms for the blog post or that can be used to create interactive content such as a glossary or explanatory videos.\\n\\nHere is how we will proceed with the improved plan:\\n\\nStep 1: Research Coordinator (RC)\\n- Update the queries sent to the arXiv API to include \"practical applications of LLM\", \"real-world impacts of LLM reliability issues\", and \"visual data on LLM safety mechanisms\".\\n- Review the summaries of the papers provided by the API to find suitable examples, diagrams, and glossary items.\\n- Share the selected research information, including accessible content and potential interactive elements with the Researcher.\\n\\nStep 2: Researcher (R)\\n- Write the blog post incorporating simple definitions of technical terms like \"parameters,\" along with practical examples and the real-world impact of reliability and safety issues in LLMs.\\n- Turn complex concepts into reader-friendly explanations, perhaps by using metaphors or analogies.\\n- Integrate infographics based on the new data provided to illustrate complicated ideas.\\n- Embed hyperlinks properly within the text for references, replacing raw links to enhance readability.\\n- Create a glossary of terms if possible, or work with a web developer to implement interactive elements that can improve understanding.\\n- Incorporate a strong call to action at the end of the blog post to engage the readers and encourage them to contribute to the conversation on AI safety and ethics.\\n\\nThis revised plan should result in a blog post that is not only informative and up-to-date with the latest research but also engaging and accessible to a non-technical audience. After implementing these changes, the plan should be submitted back to the PI for approval.', 'name': 'Planner', 'role': 'user'}, {'content': \"As the individual tasked with evaluating the Planner's revised strategy, my review is as follows:\\n\\n**Strengths of the Revised Plan:**\\n\\n1. The revised plan addresses all feedback points provided by the Critic, demonstrating a thoughtful approach to improving the reader's experience.\\n2. The inclusion of practical applications and real-world impacts, as well as the integration of infographics and interactive elements, will significantly enhance the accessibility of the blog post.\\n3. The plan encourages active engagement with the content through a strong call to action and interactive features like a glossary, making the learning process more dynamic for the readers.\\n4. Hyperlinking within the text and simplifying language as per the Critic's recommendations will undoubtedly enhance the readability and engagement level of the blog post.\\n\\n**Further Recommendations:**\\n\\n1. **Collaboration with a Designer**: The plan should mention the possible need for collaboration with a professional designer or web developer. This would facilitate the creation of high-quality visual aids and the implementation of interactive elements like videos and hover-able glossary terms.\\n\\n2. **User Feedback Loop**: The plan might also incorporate a feedback mechanism post-publication, allowing readers to ask questions or suggest further improvements to the content, thus closing the loop on reader engagement and content refinement.\\n\\n3. **SEO Considerations**: To ensure that the blog post reaches its intended audience, the plan should also factor in search engine optimization (SEO) techniques. This includes keyword research and optimization to improve discoverability of the blog post on search engines.\\n\\n4. **Legal and Ethical Review**: Given the subject matter involves ethics and governance, a review step should be included to ensure compliance with any legal considerations and that ethical guidelines are being followed in the representation of the information.\\n\\n5. **Promotion Plan**: Adding a step for promoting the blog post across various platforms can maximize reach and impact. This should involve scheduling posts on social media and possibly engaging with influencers in the AI ethics space.\\n\\nSummarizing, the revised plan is well-conceived and thorough. However, refining details such as design collaboration, SEO, and promotion can elevate the end result even further. Once these additional recommendations are considered, the plan should be ready for implementation.\", 'name': 'Critic', 'role': 'user'}, {'content': \"Taking into account the further recommendations provided by the Critic, I will adjust the revised plan as follows:\\n\\nStep 1: Research Coordinator (RC)\\n- Include queries related to the SEO aspect of the research topics to ensure that the blog post content is optimized for search engines.\\n- Coordinate with a designer or web developer for the creation of infographics and interactive elements.\\n\\nStep 2: Researcher (R)\\n- Integrate suggestions from the designer or web developer to ensure that the blog post is visually engaging and interactive for the reader.\\n- Incorporate an SEO strategy for the blog post, selecting keywords based on the queries and optimizing the content for better visibility on search engines.\\n- Draft the blog post with legal and ethical considerations in mind, possibly consulting with an expert in AI ethics to verify the content's compliance before publication.\\n\\nAdditionally, the plan will be expanded to include:\\n\\nStep 3: Post-Publication (R/RC)\\n- Establish a feedback mechanism for readers to interact with the content post-publication. This could involve comments on the blog, a dedicated email address, or social media interaction.\\n- Utilize the Research Coordinator's knowledge or collaborate with a marketing specialist to develop a promotion plan. This plan will outline how the blog post will be shared across social media platforms, potentially involve community outreach, and identify key influencers who could help disseminate the content.\\n\\nThe implementation of these additional steps into the plan will help ensure that the content is not only informative and accessible but also reaches and engages the targeted audience effectively. After careful integration of all feedback points, the plan should once again be reviewed by the PI for final approval.\", 'name': 'Planner', 'role': 'user'}, {'content': \"As the entity evaluating the Planner's adjusted strategy, I commend the incorporation of additional recommendations aimed at enhancing the impact and reach of the blog post.\\n\\n**Strong Points of the Adjusted Plan:**\\n\\n1. The addition of SEO-friendly steps is a substantial improvement, considering the importance of online visibility in today's digital landscape.\\n2. Coordination with designers and web developers to create compelling visual and interactive content shows a commitment to a high-quality reader experience.\\n3. The consideration of legal and ethical checks prior to publication demonstrates responsibility and conscientiousness concerning the sensitive nature of AI ethics.\\n4. The post-publication strategy, especially the feedback mechanism and the structured promotional strategy, ensures that engagement with the blog post continues beyond its initial release.\\n\\n**Final Recommendations:**\\n\\n1. **Quality Assurance**: Prior to publication, include a step for a final quality assurance review to ensure that all elements of the blog post, including text, visuals, and interactive features, are functioning correctly and are free of errors.\\n\\n2. **Analytics Tracking**: Implement an analytics tracking system to monitor the blog post's performance, reader engagement, and the effectiveness of the SEO strategy.\\n\\n3. **Iterative Updates**: Plan for future updates to the blog post based on new research findings, legal and ethical developments in AI, or feedback from users, keeping the content relevant and updated.\\n\\n4. **Accessibility**: Ensure that the blog post and all its associated interactive and visual elements are accessible to a wide audience, including those with disabilities.\\n\\nWith these final considerations integrated, the adjusted plan appears comprehensive and well-suited to produce a blog post that will be educational, engaging, and widely accessible. The Planner has demonstrated a thorough understanding of the objectives and has responded effectively to all feedback. The plan is recommended for final review by the PI, and upon approval, it should be ready for execution.\", 'name': 'Critic', 'role': 'user'}, {'content': \"The revised plan is well-rounded and addresses all the necessary refinements to make the blog post both informative and accessible. Engaging visual and interactive content, coupled with a robust SEO and promotion strategy, and post-publication interaction, will significantly enhance the readers experience and the blog post's reach.\\n\\nThe points on ensuring quality assurance, implementing analytics tracking, planning for iterative updates, and guaranteeing accessibility are all critical to ensure the blog post's success. Collaboration across the team, including researchers, web developers, designers, and marketing specialists, will be vital in executing this plan effectively.\\n\\nAs the Principal Investigator, I approve the adjusted plan for execution, with the note that each step is duly implemented with care and attention to detail. The Research Coordinator should oversee the coordination between different elements, whereas the Researcher must ensure that the content is accurate, up-to-date, and reflects the latest in ethical AI research. The Planner should ensure that the timeline and resources required for each step are managed efficiently.\\n\\nUpon completion of these tasks, and with the final content ready for publication, we should have a blog post that serves as a valuable resource for those interested in the intersection of AI, ethics, and technology.\\n\\nProceed with the execution of the plan, and ensure that each element aligns with the high standards we aim to maintain. Good luck with the implementation phase, and I look forward to reviewing the final product.\\n\\nTERMINATE\", 'role': 'assistant'}, {'content': 'Our work here is concluded. If further assistance is needed, feel free to reach out. Good luck with the implementation of the plan and the creation of the blog post!', 'name': 'ResearchCoordinator', 'role': 'user'}, {'content': '', 'name': 'ResearchCoordinator_proxy', 'role': 'user'}, {'content': 'TERMINATE', 'role': 'assistant'}]})\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# You MUST Collect enough information from the arxiv API and summerize the information in the blog post.\n",
    "PROBLEM = \"\"\"You are expert in reliability and safety mechanisms in AI system. Write a blog post on the topic of \"Reliability and Safety Mechanisms in AI Systems\". The blog post MUST include the following points:\n",
    "The focus MUST be on Large Language Models.\n",
    "The blog post should should have reffrences to the sources you used to perform the research and also for reader to refer for more information.\n",
    "The blog post MUST be written in a way that is easy to understand for a non-technical audience.\n",
    "The blog post MUST be up to date and include the latest research in the field in your database.\n",
    "\"\"\"\n",
    "messages = call_chat(PROBLEM)\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
