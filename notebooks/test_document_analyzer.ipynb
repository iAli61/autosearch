{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def keep_only_first_page(input_pdf_path, output_pdf_path):\n",
    "    with open(input_pdf_path, 'rb') as f_in:\n",
    "        reader = PyPDF2.PdfReader(f_in)\n",
    "        writer = PyPDF2.PdfWriter()\n",
    "        writer.add_page(reader.pages[0])\n",
    "    with open(output_pdf_path, 'wb') as f_out:\n",
    "        writer.write(f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"/home/alibina/repo/autosearch/tests/files/real_test.pdf\"\n",
    "output_pdf_file = \"/home/alibina/repo/autosearch/tests/files/real_test_output.pdf\"\n",
    "keep_only_first_page(pdf_file, output_pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed successfully.\n",
      "Analysis result saved to analysis_result.json\n",
      "Created 46 docs with a total of 13583 tokens. Largest doc has 998 tokens.\n",
      "Created 46 documents.\n",
      "First document content:\n",
      "## Abstract\n",
      "\n",
      "\n",
      "\n",
      "Recently, pre-trained language models (LMs) have achieved strong performance when fine- tuned on difficult benchmarks like Super- GLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that lever- ages patterns for few-shot learning. How- ever, PET uses task-specific unlabeled data. In this paper, we focus on few shot learn- ing without any unlabeled data and introduce ADAPET,...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "api_key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n",
    "endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "\n",
    "# Initialize the DocumentAnalyzer\n",
    "analyzer = DocumentAnalyzer(api_key, endpoint)\n",
    "\n",
    "# Analyze a PDF\n",
    "pdf_path = \"/workspaces/autogen-new/usecases/autosearch/notebooks/project_test/example_paper.pdf\"\n",
    "try:\n",
    "    result = analyzer.analyze_pdf(pdf_path)\n",
    "    print(\"Analysis completed successfully.\")\n",
    "\n",
    "    # Save the analysis result\n",
    "    analyzer.save_analysis_result(result, \"./project_test/analysis_result.json\")\n",
    "    print(\"Analysis result saved to analysis_result.json\")\n",
    "\n",
    "    # Create documents from the analysis result\n",
    "    docs, page_content, full_md_text = analyzer.create_docs(result, max_token_size=1000, source_name=\"example_paper\")\n",
    "    print(f\"Created {len(docs)} documents.\")\n",
    "\n",
    "    # Print the content of the first document\n",
    "    if docs:\n",
    "        print(\"First document content:\")\n",
    "        print(docs[0].page_content[:500] + \"...\")  # Print first 500 characters\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except HttpResponseError as e:\n",
    "    print(f\"Azure service error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
